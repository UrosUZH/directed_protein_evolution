{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b31f84c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "Device name: NVIDIA GeForce RTX 5070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3db6ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6449960283d4f42ae40659221f04a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bec8ce7c024b75b52ff54033713521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccce6f51ef94e249070005f9a302ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/31.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmForMaskedLM: ['esm.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing EsmForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e191baa54d4c0c92bbc193da28911e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421a3841cc9849ccb582efe9509c917f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7299d5e4d0c94182831d1ed6616bcdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39954ece4eda4b64963d4b8fc4ed92b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f546860df1c4f7ca3df2cde464a8a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ground truth sequence:\n",
      "SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\n",
      "\n",
      "Masked input sequence:\n",
      "<mask>KGEEL<mask><mask>G<mask>VPILVELDGDVN<mask><mask>KFSVSGEG<mask>GDAT<mask><mask>KL<mask>LK<mask>I<mask>TTG<mask>LPVPWP<mask>LVTTLS<mask><mask>VQCF<mask><mask>YPDHMKQ<mask>DFFK<mask><mask><mask>PEGYVQ<mask>R<mask>IF<mask>KDDGNY<mask><mask>R<mask>E<mask>KFEGDTLVNRIELKGI<mask>FKED<mask><mask>ILGH<mask>LEYNY<mask>SHNVYIM<mask>D<mask>QK<mask><mask>IKVNFKIR<mask>N<mask>EDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEK<mask>D<mask>MVLLEF<mask>TAAGITHGMDELYK\n",
      "\n",
      "Predicted sequence:\n",
      "MKGEELKEGDVPILVELDGDVNSLKFSVSGEGKGDATGLKLELKGIDTTGELPVPWPELVTTLSGGVQCFSDYPDHMKQLDFFKKLLPEGYVQGRGIFKKDDGNYLLRLELKFEGDTLVNRIELKGIDFKEDGKILGHKLEYNYDSHNVYIMKDGQKKPIKVNFKIRLNKEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKLDGMVLLEFLTAAGITHGMDELYK\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, # If we only need the embeddings\n",
    "    AutoModelForMaskedLM # If we want to work with the masked LM embeddings (additional logits output)\n",
    ")\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random \n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu2'\n",
    "print(f\"Using device: {device}\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "# Load ESM2 8M model and tokenizer\n",
    "model_checkpoint = \"facebook/esm2_t6_8M_UR50D\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint, force_download=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, force_download=True)\n",
    "\n",
    "# TODO: for later usage with LeoMed, save your model (and tokenizer) and scp this directory to the server\n",
    "# Then set force_download=False if you specify a local path to load from \n",
    "# path_to_save = ...\n",
    "# model.save_pretrained(path_to_save) \n",
    "\n",
    "\n",
    "# Load sample DMS data\n",
    "df = pd.read_csv('data/gfp_ground_truth.csv')\n",
    "\n",
    "# Extract sample from df and mask random 15%\n",
    "seq = df['sequence'][0]\n",
    "mask_prob = 0.15\n",
    "masked_chars = []\n",
    "for aa in seq:\n",
    "    if random.random() < mask_prob:\n",
    "        masked_chars.append(tokenizer.mask_token)\n",
    "    else:\n",
    "        masked_chars.append(aa)\n",
    "masked_seq = \"\".join(masked_chars)\n",
    "\n",
    "# Tokenize and input to ESM2\n",
    "inputs = tokenizer(masked_seq, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # (1, seq_len, vocab_size)\n",
    "\n",
    "# Decode predictions by taking argmax\n",
    "pred_ids = logits.argmax(dim=-1)  # (1, seq_len)\n",
    "pred_seq = tokenizer.decode(\n",
    "    pred_ids[0],\n",
    "    skip_special_tokens=True\n",
    ").replace(\" \", \"\")\n",
    "\n",
    "print(\"\\nGround truth sequence:\")\n",
    "print(seq)\n",
    "print(\"\\nMasked input sequence:\")\n",
    "print(masked_seq)\n",
    "print(\"\\nPredicted sequence:\")\n",
    "print(pred_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8da073b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = df['sequence'][0]\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb614a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mask>KG<mask>ELFTGVVPILVE<mask>D<mask>DVNG<mask>KFSV<mask>GEGEG<mask>ATYGKL<mask>LKFICT<mask>GKLPV<mask>WPTLVTTLSYGV<mask>CFSRYPDHMKQHDFFK<mask>AMPEGYVQ<mask>RTI<mask>FKDDGNY<mask><mask>RA<mask>VKFEGDTLVNR<mask>E<mask>KGIDFKEDGNILGHKLEYNY<mask>SHN<mask>YIMADKQKNGIK<mask>NF<mask>IRHNIE<mask>GSVQ<mask>ADHYQQNTP<mask>GDGP<mask>L<mask>PD<mask>HYLS<mask>QSA<mask>SKDPNEKRDHMVL<mask>EFVT<mask><mask>G<mask>THGMDEL<mask>K'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_prob = 0.15\n",
    "masked_chars = []\n",
    "for aa in seq:\n",
    "    if random.random() < mask_prob:\n",
    "        masked_chars.append(tokenizer.mask_token)\n",
    "    else:\n",
    "        masked_chars.append(aa)\n",
    "masked_seq = \"\".join(masked_chars)\n",
    "masked_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "761aadcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 13.9560,  -7.6173,  -5.9903,  ..., -15.5301, -15.7619,  -7.6145],\n",
       "         [ -7.2889, -14.7171,  -7.4395,  ..., -15.7856, -15.9946, -14.7180],\n",
       "         [-11.6578, -20.0635, -12.1473,  ..., -16.2404, -16.1713, -20.0570],\n",
       "         ...,\n",
       "         [-10.9424, -17.2669,  -9.6918,  ..., -16.1643, -16.1947, -17.2611],\n",
       "         [-10.8844, -17.1586, -10.6407,  ..., -16.1206, -16.0598, -17.1437],\n",
       "         [ -6.1914,  -6.8257,  16.5295,  ..., -16.7358, -16.6179,  -6.8626]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(masked_seq, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # (1, seq_len, vocab_size)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0147a3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 20, 15,  6, 15,  9,  4, 18, 11,  6,  7,  7, 14, 12,  4,  7,  9,  6,\n",
       "         13, 12, 13,  7, 17,  6,  4, 15, 18,  8,  7, 15,  6,  9,  6,  9,  6, 15,\n",
       "          5, 11, 19,  6, 15,  4, 15,  4, 15, 18, 12, 23, 11, 13,  6, 15,  4, 14,\n",
       "          7, 13, 22, 14, 11,  4,  7, 11, 11,  4,  8, 19,  6,  7,  6, 23, 18,  8,\n",
       "         10, 19, 14, 13, 21, 20, 15, 16, 21, 13, 18, 18, 15, 15,  5, 20, 14,  9,\n",
       "          6, 19,  7, 16,  9, 10, 11, 12,  9, 18, 15, 13, 13,  6, 17, 19,  9,  7,\n",
       "         10,  5,  9,  7, 15, 18,  9,  6, 13, 11,  4,  7, 17, 10, 12,  9, 12, 15,\n",
       "          6, 12, 13, 18, 15,  9, 13,  6, 17, 12,  4,  6, 21, 15,  4,  9, 19, 17,\n",
       "         19, 13,  8, 21, 17,  4, 19, 12, 20,  5, 13, 15, 16, 15, 17,  6, 12, 15,\n",
       "          4, 17, 18, 11, 12, 10, 21, 17, 12,  9, 13,  6,  8,  7, 16, 12,  5, 13,\n",
       "         21, 19, 16, 16, 17, 11, 14,  4,  6, 13,  6, 14,  4,  4,  4, 14, 13,  6,\n",
       "         21, 19,  4,  8, 12, 16,  8,  5,  4,  8, 15, 13, 14, 17,  9, 15, 10, 13,\n",
       "         21, 20,  7,  4,  7,  9, 18,  7, 11, 15, 13,  6, 15, 11, 21,  6, 20, 13,\n",
       "          9,  4, 15, 15,  2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ids = logits.argmax(dim=-1)  # (1, seq_len)\n",
    "pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3d0c1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MKGKELFTGVVPILVEGDIDVNGLKFSVKGEGEGKATYGKLKLKFICTDGKLPVDWPTLVTTLSYGVGCFSRYPDHMKQHDFFKKAMPEGYVQERTIEFKDDGNYEVRAEVKFEGDTLVNRIEIKGIDFKEDGNILGHKLEYNYDSHNLYIMADKQKNGIKLNFTIRHNIEDGSVQIADHYQQNTPLGDGPLLLPDGHYLSIQSALSKDPNEKRDHMVLVEFVTKDGKTHGMDELKK'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_seq = tokenizer.decode(\n",
    "    pred_ids[0],\n",
    "    skip_special_tokens=True\n",
    ").replace(\" \", \"\")\n",
    "pred_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205edbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decode predictions by taking argmax\n",
    "pred_ids = logits.argmax(dim=-1)  # (1, seq_len)\n",
    "pred_seq = tokenizer.decode(\n",
    "    pred_ids[0],\n",
    "    skip_special_tokens=True\n",
    ").replace(\" \", \"\")\n",
    "\n",
    "print(\"\\nGround truth sequence:\")\n",
    "print(seq)\n",
    "print(\"\\nMasked input sequence:\")\n",
    "print(masked_seq)\n",
    "print(\"\\nPredicted sequence:\")\n",
    "print(pred_seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
